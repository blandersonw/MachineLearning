{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMx5l1Cb8WtLfLmj6ce7S7a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blandersonw/MachineLearning/blob/main/HW10_BlakeAnderson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this homework, we will implement XOR by training the two-layer Multilayer Perceptron as shown in the lecture note.\n",
        "\n",
        "Use seed=0 whenever possible.\n",
        "\n",
        "1. First consider four training examples X=[[0,0],[0,1],[1,0],[1,1]]. What should their ground truth outputs y be ?\n",
        "\n",
        "2. Now import \"MLPClassifier\" from \"sklearn.neural_network\".\n",
        "\n",
        "To instantiate the class, the key hyperparameters that have to specified are \"hidden_layer_sizes\", \"activation\", and \"learning_rate_init\".  (Read https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html Links to an external site.)\n",
        "\n",
        "The model shown in class uses \"hidden_layer_sizes=[2]\" and the step function as activation. However we need a differentiable activation function, so use \"activation='logistic'\". Also use \"random_state=0, max_iter=500, alpha=0.01\".\n",
        "\n",
        "3. Use \"learning_rate_init\" from [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5]. Instantiate the model and fit the model to the training data. For each of the learning rate, report the training accuracy.\n",
        "\n",
        "4. With the best rate, retrain the model. What are the learned weights? They should be a total of 9 numbers, as you can see from the picture in the lecture note. (Hint: the weights are stored in \"coefs_\", and \"intercepts_\")."
      ],
      "metadata": {
        "id": "lzltSZ0PAJaD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NtuPptOVAI2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b84016-df64-4207-ac2d-a8bc72438296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate: 0.001 has training accuracy 0.5\n",
            "Learning rate: 0.005 has training accuracy 0.5\n",
            "Learning rate: 0.01 has training accuracy 0.75\n",
            "Learning rate: 0.05 has training accuracy 0.5\n",
            "Learning rate: 0.1 has training accuracy 0.5\n",
            "Learning rate: 0.5 has training accuracy 1.0\n",
            "\n",
            "The learned weights for learning rate 0.5 (highest accuracy of 1.0) are... \n",
            "\n",
            "Coefficients: \n",
            " [[ 4.82877822, -3.91633033],\n",
            " [ 4.80462945, -3.92319884]]\n",
            "[[-7.71435547],\n",
            " [-6.76571913]]\n",
            "Intercepts: \n",
            " [-8.1611431 ,  1.82010429]\n",
            "[3.66602896]\n",
            "\n",
            "\n",
            "Learning rate: 1 has training accuracy 0.75\n",
            "Learning rate: 5 has training accuracy 0.5\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "#training examples\n",
        "X=[[0,0],[0,1],[1,0],[1,1]]\n",
        "\n",
        "#the expected output for XOR\n",
        "y = [0,1,1,0]\n",
        "\n",
        "#creating a function to run the MLPmodel for each of the learning rates\n",
        "def MLPmodel(k):\n",
        "\n",
        "  #instantiate model with k learning rate\n",
        "  MLP = MLPClassifier(hidden_layer_sizes =[2],  activation = 'logistic', learning_rate_init = k, random_state=0, max_iter=500, alpha=0.01).fit(X,y)\n",
        "\n",
        "  #calculate accuracy on training data\n",
        "  train_acc = MLP.score(X, y)\n",
        "\n",
        "  #print results\n",
        "  print(f\"Learning rate: {k} has training accuracy {train_acc}\")\n",
        "\n",
        "  #we found optimal learning rate is .5, so we will print out the learned rates corresponding to learning rate .5\n",
        "  if k == .5:\n",
        "    print(f\"\\nThe learned weights for learning rate {k} (highest accuracy of {train_acc}) are... \\n\")\n",
        "    print(\"Coefficients: \\n\", np.array2string(MLP.coefs_[0], separator=', '))\n",
        "    print(np.array2string(MLP.coefs_[1], separator=', ')) \n",
        "    print(\"Intercepts: \\n\", np.array2string(MLP.intercepts_[0], separator=', '))\n",
        "    print(np.array2string(MLP.intercepts_[1], separator=', '))  \n",
        "    print(\"\\n\")\n",
        "\n",
        "learning_rate = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5]\n",
        "\n",
        "#running function for each learning rate\n",
        "for k in learning_rate:\n",
        "  MLPmodel(k)"
      ]
    }
  ]
}